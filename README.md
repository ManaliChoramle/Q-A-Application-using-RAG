# Employee HR Question Answer Application using RAG Architecture 

**Overview**

This project involves the development of an Employee HR Q&A application utilizing a Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) techniques. The application aims to enhance employee experience by providing quick and accurate responses to HR-related queries. It leverages LLM capabilities to generate human-like answers and RAG to retrieve relevant information from a knowledge base, ensuring that employees receive contextually appropriate and precise responses.

**Need for the Project**

The HR department often receives numerous queries related to policies, benefits, and procedures. Traditional methods of addressing these queries can be time-consuming and inefficient. This application addresses the need for:

Quick Information Retrieval: Employees can get immediate answers to their HR-related questions.
Increased Efficiency: Reduces the workload on HR personnel by automating response generation.
Enhanced Employee Experience: Provides a user-friendly interface for employees to access vital HR information effortlessly.

**Tech Stack**

Programming Language: Python
Libraries/Tools:
AWS Bedrock: Used for integrating pre-trained models and building the application with scalable infrastructure.
Claude Foundation Model (FM): Utilized as the underlying Large Language Model (LLM) to generate contextually relevant responses.
FAISS Vector Store: Implemented for efficient similarity search and retrieval of relevant documents from the knowledge base.
LangChain: Employed for building the Retrieval-Augmented Generation (RAG) system, facilitating seamless interaction between the retrieval and generation components.
Prompt Tuning & Engineering: Techniques applied to enhance the quality of responses generated by the LLM, ensuring they are tailored to user queries.

**How It Works**

Data Source:  HR policy PDF to create a knowledge base.
Retrieval-Augmented Generation: The RAG model retrieves relevant documents based on user queries and passes them to the LLM for context-aware answer generation.
API Development: The application is served through a AWS API gateway, enabling users to submit queries and receive responses.
Frontend Interface: A simple web interface allows employees to enter their questions and view the generated responses.
